{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Define the Ollama API endpoint\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the model you want to use\n",
    "model_name = \"gemma3:1b\"  # Replace with the actual model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model_name, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Ollama API and return the generated response.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,  # Set to True if you want streaming responses\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    response = requests.post(OLLAMA_API_URL, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"response\", \"\").strip()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imputed_value(response):\n",
    "    \"\"\"\n",
    "    Extracts the imputed value from the AI response.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"ANSWER:\\s*(.*)\", response)\n",
    "    return match.group(1) if match else None  # Return None if no match found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_imputation(row, missing_column):\n",
    "    \"\"\"\n",
    "    Impute missing values in a row using the Ollama API.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a database engineer, also knowledgeble in movies and IMDB. Fill in the missing value.\n",
    "\n",
    "    Record:\n",
    "    {row.to_dict()}\n",
    "\n",
    "    What is the best guess for '{missing_column}'? \n",
    "    Provide your reasoning first, then state your final answer in the format: `ANSWER: <your answer>`.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = generate_response(prompt, model_name)\n",
    "     \n",
    "\n",
    "    return extract_imputed_value(response) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, the current version of prompt makes stupid prediction, but out in a good format. \\\n",
    "To do ;\n",
    "- few shot prompting \n",
    "- change prompt's persona assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df = pd.read_csv(\"testDI.csv\")  # Load your dataset\n",
    "missing_col = \"originalTitle\"  # Column with missing values\n",
    "\n",
    "# Apply data imputation to rows with missing values\n",
    "df[missing_col] = df.apply(\n",
    "    lambda row: data_imputation(row, missing_col) if pd.isnull(row[missing_col]) else row[missing_col],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the preprocessed data\n",
    "df.to_csv(\"ImputedOriginalTitle_allOutput.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 rows of imputation on laptop = 9 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Batch Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Imputation Progress:   0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\n.jintaganon\\AppData\\Local\\Temp\\ipykernel_25236\\562484669.py:42: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Thé Gớd ớf Cớớkéry**' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.at[index, missing_column] = imputed_value  # Fill missing value\n",
      "Batch Imputation Progress: 100%|██████████| 1/1 [05:00<00:00, 300.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed data saved as 'imputed_data.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to extract only the imputed value\n",
    "def extract_imputed_value(response):\n",
    "    match = re.search(r\"ANSWER:\\s*(.*)\", response)\n",
    "    return match.group(1) if match else None  # Return None if no match found\n",
    "\n",
    "# Function to perform imputation\n",
    "def data_imputation(row, missing_column):\n",
    "    prompt = f\"\"\"\n",
    "    You are an IMDB expert. You know all the details of the movies listed in IMDB. Fill in the missing value.\n",
    "\n",
    "    Record:\n",
    "    {row.to_dict()}\n",
    "\n",
    "    What is the best guess for '{missing_column}'? \n",
    "    Provide your reasoning first, then state your final answer in the format: `ANSWER: <your answer>`.\n",
    "    \"\"\"\n",
    "    response = generate_response(prompt, model_name)  # Call your LLM function\n",
    "    return extract_imputed_value(response)  # Extract imputed value\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"testDI.csv\")\n",
    "\n",
    "# Define the column with missing values\n",
    "missing_column = \"originalTitle\"\n",
    "\n",
    "# Identify rows with missing values\n",
    "missing_indices = df[df[missing_column].isna()].index\n",
    "BATCH_SIZE = 20  # Define batch size\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(0, len(missing_indices), BATCH_SIZE), desc=\"Batch Imputation Progress\"):\n",
    "    batch_indices = missing_indices[i:i + BATCH_SIZE]  # Select batch\n",
    "    batch_data = df.loc[batch_indices]  # Extract batch\n",
    "\n",
    "    for index, row in batch_data.iterrows():\n",
    "        imputed_value = data_imputation(row, missing_column)\n",
    "        df.at[index, missing_column] = imputed_value  # Fill missing value\n",
    "\n",
    "    # Save progress after each batch\n",
    "    df.to_csv(\"imputed_data.csv\", index=False)\n",
    "\n",
    "    # Optional: Avoid API rate limits\n",
    "    time.sleep(2)  # Adjust based on API limits\n",
    "\n",
    "print(\"Imputed data saved as 'imputed_data.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch processing, model's processing time goes down from 9 mins to 5 mins.\n",
    "Changing the prompt doesn't help much. This is more like copying the primaryTitle. But this might be due to small sample size.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
